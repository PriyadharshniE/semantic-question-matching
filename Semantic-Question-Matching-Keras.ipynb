{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic-Question-Matching-Keras\n",
    "\n",
    "You can download data from: http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv \n",
    "\n",
    "Dataset info: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs \n",
    "\n",
    "Blog post about Quora model: https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import sqmutils.data_utils as du\n",
    "import time\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "pathToDataset = \"/home/elkhand/datasets/Quora/data/quora_duplicate_questions.tsv\"\n",
    "\n",
    "# embedding_path = \"/home/elkhand/datasets/glove-vectors/glove.twitter.27B.100d.txt\"\n",
    "# emb_dim = 100\n",
    "\n",
    "#embedding_path = \"/home/elkhand/datasets/glove-vectors/glove.840B.300d.txt\"\n",
    "#emb_dim = 300\n",
    "\n",
    "# https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "embedding_path = \"/home/elkhand/datasets/fasttext/wiki.en.vec\"\n",
    "emb_dim = 300\n",
    "\n",
    "\n",
    "train_dataset_path = pathToDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(pathToDataset, sep='\\t', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/dev/test set\n",
    "- train 98%\n",
    "- dev 1%\n",
    "- test 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n",
      " {'train_dataset_path': '/home/elkhand/datasets/Quora/data/quora_duplicate_questions.tsv', 'test_size': 0.01, 'val_size': 0.01, 'max_seq_len': 32, 'embedding_dimension': 300, 'batch_size': 3096, 'nb_epochs': 100, 'recurrent_dropout': 0.3, 'dropout': 0.3, 'seed': 7, 'is_debug_on': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " trainDataset Label distribution:  is_duplicate\n",
      "0    249952\n",
      "1    146292\n",
      "Name: is_duplicate, dtype: int64 \n",
      "\n",
      "\n",
      " valDataset Label distribution:  is_duplicate\n",
      "0    2525\n",
      "1    1478\n",
      "Name: is_duplicate, dtype: int64 \n",
      "\n",
      "\n",
      " testDataset Label distribution:  is_duplicate\n",
      "0    2550\n",
      "1    1493\n",
      "Name: is_duplicate, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First split dataset into train(99%) and test(1%)\n",
    "test_size = 0.01 # 0.2\n",
    "val_size = 0.01 # 0.2\n",
    "\n",
    "config = du.get_config(train_dataset_path, test_size=test_size, val_size=val_size, embedding_dimension=emb_dim)\n",
    "trainDataset, testDataset = du.create_train_test_split(config)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Second, split train dataset into train (98%) and val (1%) datasets\n",
    "trainDataset, valDataset = du.create_train_test_split_from_df(trainDataset, config, isValSplit=True)\n",
    "\n",
    "\n",
    "print(\"\\n\",\"trainDataset Label distribution: \", trainDataset.groupby('is_duplicate').is_duplicate.count(), \"\\n\")\n",
    "print(\"\\n\",\"valDataset Label distribution: \", valDataset.groupby('is_duplicate').is_duplicate.count() , \"\\n\")\n",
    "print(\"\\n\",\"testDataset Label distribution: \", testDataset.groupby('is_duplicate').is_duplicate.count() , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why does yellow mustard relieve a burn</td>\n",
       "      <td>What is the difference between Dijon mustard a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What startup companies are hiring in Bangalore</td>\n",
       "      <td>Are there any startups hiring in Bangalore</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the Bermuda Triangle</td>\n",
       "      <td>What do you think about the Bermuda Triangle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some fun things to do</td>\n",
       "      <td>What are some fun things to do in Delhi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I find the mole weight of air if I don...</td>\n",
       "      <td>How do I plan a 10 day trip to Norway from Mum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0            Why does yellow mustard relieve a burn    \n",
       "1    What startup companies are hiring in Bangalore    \n",
       "2                      What is the Bermuda Triangle    \n",
       "3                    What are some fun things to do    \n",
       "4  How can I find the mole weight of air if I don...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the difference between Dijon mustard a...             0  \n",
       "1        Are there any startups hiring in Bangalore              1  \n",
       "2      What do you think about the Bermuda Triangle              0  \n",
       "3           What are some fun things to do in Delhi              0  \n",
       "4  How do I plan a 10 day trip to Norway from Mum...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings\n",
    "\n",
    "We will be using Fasttext Wiki word vectors 300D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors path /home/elkhand/datasets/fasttext/wiki.en.vec\n",
      "embedding size : 2519371\n",
      "embedding dimension : (300,)\n",
      "Total time passed:  320.8339681625366\n"
     ]
    }
   ],
   "source": [
    "print(\"word vectors path\", embedding_path)\n",
    "start = time.time()\n",
    "w2v = du.load_embedding(embedding_path)\n",
    "end = time.time()\n",
    "print(\"Total time passed: \", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "\n",
    "# num_cores = 8\n",
    "# GPU=True\n",
    "# CPU = not GPU\n",
    "\n",
    "# if GPU:\n",
    "#     num_GPU = 1\n",
    "#     num_CPU = 8\n",
    "# if CPU:\n",
    "#     num_CPU = 8\n",
    "#     num_GPU = 0\n",
    "\n",
    "# configProto = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "#         inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "#         device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "# session = tf.Session(config=configProto)\n",
    "# K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time passed 177.29485511779785\n",
      "df_train_q1_emb.shape (396244, 32, 300)\n",
      "df_train_q2_emb.shape (396244, 32, 300)\n",
      "df_train_label.shape (396244,)\n",
      "df_val_q1_emb.shape (4003, 32, 300)\n",
      "df_val_q2_emb.shape (4003, 32, 300)\n",
      "df_val_label.shape (4003,)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_train_q1_emb,df_train_q2_emb, df_train_label  = du.load_dataset(trainDataset,w2v,config)\n",
    "df_val_q1_emb,df_val_q2_emb, df_val_label  = du.load_dataset(valDataset,w2v, config)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time passed\", (end - start))\n",
    "print(\"df_train_q1_emb.shape\",df_train_q1_emb.shape)\n",
    "print(\"df_train_q2_emb.shape\", df_train_q2_emb.shape)\n",
    "print(\"df_train_label.shape\", df_train_label.shape)\n",
    "    \n",
    "print(\"df_val_q1_emb.shape\",df_val_q1_emb.shape)\n",
    "print(\"df_val_q2_emb.shape\", df_val_q2_emb.shape)\n",
    "print(\"df_val_label.shape\", df_val_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit amount of data to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time passed 177.29485511779785\n",
      "df_train_q1_emb.shape (396244, 32, 300)\n",
      "df_train_q2_emb.shape (396244, 32, 300)\n",
      "df_train_label.shape (396244,)\n",
      "df_val_q1_emb.shape (4003, 32, 300)\n",
      "df_val_q2_emb.shape (4003, 32, 300)\n",
      "df_val_label.shape (4003,)\n"
     ]
    }
   ],
   "source": [
    "limit = 100000\n",
    "\n",
    "# df_train_q1_emb_limit = df_train_q1_emb[:limit,:,:]\n",
    "# df_train_q2_emb_limit = df_train_q2_emb[:limit,:,:]\n",
    "# df_train_label_limit  = df_train_label[:limit]\n",
    "\n",
    "# df_val_q1_emb_limit = df_val_q1_emb[:limit,:,:]\n",
    "# df_val_q2_emb_limit = df_val_q2_emb[:limit,:,:]\n",
    "# df_val_label_limit = df_val_label[:limit]\n",
    "\n",
    "\n",
    "df_train_q1_emb_limit = df_train_q1_emb\n",
    "df_train_q2_emb_limit = df_train_q2_emb\n",
    "df_train_label_limit  = df_train_label\n",
    "\n",
    "df_val_q1_emb_limit = df_val_q1_emb\n",
    "df_val_q2_emb_limit = df_val_q2_emb\n",
    "df_val_label_limit = df_val_label\n",
    "\n",
    "\n",
    "print(\"Total time passed\", (end - start))\n",
    "print(\"df_train_q1_emb.shape\",df_train_q1_emb_limit.shape)\n",
    "print(\"df_train_q2_emb.shape\", df_train_q2_emb_limit.shape)\n",
    "print(\"df_train_label.shape\", df_train_label_limit.shape)\n",
    "    \n",
    "print(\"df_val_q1_emb.shape\",df_val_q1_emb_limit.shape)\n",
    "print(\"df_val_q2_emb.shape\", df_val_q2_emb_limit.shape)\n",
    "print(\"df_val_label.shape\", df_val_label_limit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n",
      " {'train_dataset_path': '/home/elkhand/datasets/Quora/data/quora_duplicate_questions.tsv', 'test_size': 0.01, 'val_size': 0.01, 'max_seq_len': 32, 'embedding_dimension': 300, 'batch_size': 3096, 'nb_epochs': 100, 'recurrent_dropout': 0.3, 'dropout': 0.3, 'seed': 7, 'is_debug_on': False} \n",
      "\n",
      "Total time passed 4.76837158203125e-07\n",
      "df_train_q1_emb.shape (396244, 32, 300)\n",
      "df_train_q2_emb.shape (396244, 32, 300)\n",
      "df_train_label.shape (396244,)\n",
      "df_val_q1_emb.shape (4003, 32, 300)\n",
      "df_val_q2_emb.shape (4003, 32, 300)\n",
      "df_val_label.shape (4003,)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1_in (InputLayer)              (None, 32, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2_in (InputLayer)              (None, 32, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_23 (Masking)            (None, 32, 300)      0           q1_in[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "masking_24 (Masking)            (None, 32, 300)      0           q2_in[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 256)          439296      masking_23[0][0]                 \n",
      "                                                                 masking_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 256)          439296      masking_24[0][0]                 \n",
      "                                                                 masking_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_34 (Subtract)          (None, 256)          0           bidirectional_23[0][0]           \n",
      "                                                                 bidirectional_23[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "subtract_35 (Subtract)          (None, 256)          0           bidirectional_24[1][0]           \n",
      "                                                                 bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_67 (Multiply)          (None, 256)          0           subtract_34[0][0]                \n",
      "                                                                 subtract_34[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_68 (Multiply)          (None, 256)          0           bidirectional_23[0][0]           \n",
      "                                                                 bidirectional_23[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_69 (Multiply)          (None, 256)          0           subtract_35[0][0]                \n",
      "                                                                 subtract_35[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_70 (Multiply)          (None, 256)          0           bidirectional_24[1][0]           \n",
      "                                                                 bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 2048)         0           bidirectional_23[0][0]           \n",
      "                                                                 bidirectional_23[1][0]           \n",
      "                                                                 multiply_67[0][0]                \n",
      "                                                                 multiply_68[0][0]                \n",
      "                                                                 multiply_69[0][0]                \n",
      "                                                                 multiply_70[0][0]                \n",
      "                                                                 bidirectional_24[0][0]           \n",
      "                                                                 bidirectional_24[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 2048)         0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 256)          524544      dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 256)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 128)          32896       dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 128)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            129         dropout_36[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,436,161\n",
      "Trainable params: 1,436,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 396244 samples, validate on 4003 samples\n",
      "Epoch 1/100\n",
      "396244/396244 [==============================] - 81s 205us/step - loss: 0.5753 - acc: 0.6801 - f1: 0.4293 - recall: 0.3916 - precision: 0.6014 - val_loss: 0.4854 - val_acc: 0.7614 - val_f1: 0.7100 - val_recall: 0.7915 - val_precision: 0.6440\n",
      "Epoch 2/100\n",
      "396244/396244 [==============================] - 65s 163us/step - loss: 0.4795 - acc: 0.7596 - f1: 0.6675 - recall: 0.6628 - precision: 0.6842 - val_loss: 0.4297 - val_acc: 0.7962 - val_f1: 0.7297 - val_recall: 0.7460 - val_precision: 0.7141\n",
      "Epoch 3/100\n",
      "396244/396244 [==============================] - 66s 165us/step - loss: 0.4426 - acc: 0.7837 - f1: 0.7089 - recall: 0.7173 - precision: 0.7057 - val_loss: 0.4158 - val_acc: 0.8074 - val_f1: 0.7580 - val_recall: 0.8179 - val_precision: 0.7065\n",
      "Epoch 4/100\n",
      "396244/396244 [==============================] - 62s 156us/step - loss: 0.4177 - acc: 0.7994 - f1: 0.7324 - recall: 0.7458 - precision: 0.7227 - val_loss: 0.3884 - val_acc: 0.8161 - val_f1: 0.7585 - val_recall: 0.7832 - val_precision: 0.7354\n",
      "Epoch 5/100\n",
      "396244/396244 [==============================] - 61s 153us/step - loss: 0.4007 - acc: 0.8098 - f1: 0.7469 - recall: 0.7628 - precision: 0.7343 - val_loss: 0.3725 - val_acc: 0.8266 - val_f1: 0.7706 - val_recall: 0.7890 - val_precision: 0.7532\n",
      "Epoch 6/100\n",
      "396244/396244 [==============================] - 62s 158us/step - loss: 0.3855 - acc: 0.8185 - f1: 0.7592 - recall: 0.7776 - precision: 0.7444 - val_loss: 0.3720 - val_acc: 0.8294 - val_f1: 0.7670 - val_recall: 0.7612 - val_precision: 0.7731\n",
      "Epoch 7/100\n",
      "396244/396244 [==============================] - 68s 171us/step - loss: 0.3709 - acc: 0.8266 - f1: 0.7705 - recall: 0.7908 - precision: 0.7535 - val_loss: 0.3534 - val_acc: 0.8411 - val_f1: 0.7962 - val_recall: 0.8410 - val_precision: 0.7560\n",
      "Epoch 8/100\n",
      "396244/396244 [==============================] - 67s 168us/step - loss: 0.3588 - acc: 0.8339 - f1: 0.7809 - recall: 0.8034 - precision: 0.7616 - val_loss: 0.3496 - val_acc: 0.8461 - val_f1: 0.7929 - val_recall: 0.7979 - val_precision: 0.7881\n",
      "Epoch 9/100\n",
      "396244/396244 [==============================] - 68s 170us/step - loss: 0.3458 - acc: 0.8413 - f1: 0.7910 - recall: 0.8150 - precision: 0.7702 - val_loss: 0.3416 - val_acc: 0.8501 - val_f1: 0.8062 - val_recall: 0.8451 - val_precision: 0.7709\n",
      "Epoch 10/100\n",
      "396244/396244 [==============================] - 66s 166us/step - loss: 0.3338 - acc: 0.8479 - f1: 0.7999 - recall: 0.8247 - precision: 0.7785 - val_loss: 0.3371 - val_acc: 0.8476 - val_f1: 0.8049 - val_recall: 0.8524 - val_precision: 0.7626\n",
      "Epoch 11/100\n",
      "396244/396244 [==============================] - 72s 181us/step - loss: 0.3232 - acc: 0.8543 - f1: 0.8082 - recall: 0.8333 - precision: 0.7863 - val_loss: 0.3371 - val_acc: 0.8566 - val_f1: 0.8077 - val_recall: 0.8160 - val_precision: 0.7998\n",
      "Epoch 12/100\n",
      "201240/396244 [==============>...............] - ETA: 33s - loss: 0.3114 - acc: 0.8592 - f1: 0.8147 - recall: 0.8403 - precision: 0.7924"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ff4710dfe531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ff4710dfe531>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                   \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_val_q1_emb_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_val_q2_emb_limit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val_label_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                   callbacks=callback_list)#\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import multiply\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Masking\n",
    "\n",
    "\n",
    "# Create models dir if it does not exist\n",
    "model_dir = \"models\"\n",
    "du.create_dir(model_dir)\n",
    "\n",
    "\n",
    "def _build_model(config, model_dir, drop_rate=0.3):\n",
    "    \n",
    "    shared_lstm1 = keras.layers.Bidirectional(LSTM(128), merge_mode='concat', weights=None)\n",
    "    shared_lstm2 = keras.layers.Bidirectional(LSTM(128), merge_mode='concat', weights=None)\n",
    "    \n",
    "    input1 = keras.layers.Input(shape=(config[\"max_seq_len\"],config[\"embedding_dimension\"], ), name='q1_in')\n",
    "    mask1 = Masking(mask_value=0.0)(input1)\n",
    "    # Passing through the 1st shared lstm\n",
    "    # This time shared_lstm1 learns on q1_in\n",
    "    x1 = shared_lstm1(mask1)\n",
    "    \n",
    "    input2 = keras.layers.Input(shape=(config[\"max_seq_len\"],config[\"embedding_dimension\"], ), name='q2_in')\n",
    "    mask2 = Masking(mask_value=0.0)(input2)\n",
    "    \n",
    "    # This time shared_lstm1 learns on q2_in\n",
    "    x2 = shared_lstm1(mask2)\n",
    "    \n",
    "    # Passing through 2nd shared lstm\n",
    "    # This time shared_lstm2 learns on q2_in, and then on q1_in\n",
    "    x22 = shared_lstm2(mask2)\n",
    "    x11 = shared_lstm2(mask1)\n",
    "    \n",
    "    # Equivalent to subtracted = keras.layers.subtract([x1, x2])\n",
    "    subtracted = keras.layers.Subtract()([x1, x2])\n",
    "    subtracted = multiply([subtracted, subtracted])\n",
    "    \n",
    "    mult = multiply([x1, x2])\n",
    "    #dotProduct = keras.layers.dot([x1, x2], axes = -1,  normalize=False)\n",
    "    \n",
    "    \n",
    "    subtracted2 = keras.layers.Subtract()([x11, x22])\n",
    "    subtracted2 = multiply([subtracted2, subtracted2])\n",
    "    \n",
    "    mult2 = multiply([x11, x22])\n",
    "    #dotProduct2 = keras.layers.dot([x11, x22], axes = -1,  normalize=False)\n",
    "    \n",
    "    diffSubtracted = keras.layers.Subtract()([subtracted, subtracted2])\n",
    "    diffSubtractedMult =  multiply([diffSubtracted, diffSubtracted])\n",
    "    subtractMult = multiply([subtracted, subtracted2])\n",
    "    \n",
    "    concatenate = keras.layers.concatenate([x1,x2, subtracted, mult,# diffSubtracted, diffSubtractedMult, subtractMult, \n",
    "                                            subtracted2, mult2, x22, x11], axis=-1)\n",
    "    concatenate = Dropout(drop_rate)(concatenate)\n",
    "    #concatenate = keras.layers.concatenate([x1,x2], axis=-1)\n",
    "    \n",
    "    dense1 = Dense(256, activation='relu', name='dense1')(concatenate)\n",
    "    dense1 = Dropout(drop_rate)(dense1)\n",
    "    \n",
    "    dense2 = Dense(128, activation='relu', name='dense2')(dense1)\n",
    "    dense2 = Dropout(drop_rate)(dense2)\n",
    "    \n",
    "    output = Dense(1, activation='sigmoid', name='output')(dense2)\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = Model(inputs=[input1, input2], outputs=[output])\n",
    "    \n",
    "    parallel_model = multi_gpu_model(model, gpus=2)\n",
    "    optimizer = keras.optimizers.Nadam()\n",
    "    #optimizer = keras.optimizers.Adam()\n",
    "    parallel_model.compile(loss='binary_crossentropy', optimizer=optimizer, \n",
    "                  metrics=['accuracy', du.f1, du.recall, du.precision])\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file= os.path.join(model_dir,\"model_architecture.png\"))\n",
    "    return parallel_model\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    start = time.time()\n",
    "    end = time.time()\n",
    "    print(\"Total time passed\", (end - start))\n",
    "    print(\"df_train_q1_emb.shape\",df_train_q1_emb_limit.shape)\n",
    "    print(\"df_train_q2_emb.shape\", df_train_q2_emb_limit.shape)\n",
    "    print(\"df_train_label.shape\", df_train_label_limit.shape)\n",
    "    \n",
    "    print(\"df_val_q1_emb.shape\",df_val_q1_emb_limit.shape)\n",
    "    print(\"df_val_q2_emb.shape\", df_val_q2_emb_limit.shape)\n",
    "    print(\"df_val_label.shape\", df_val_label_limit.shape)\n",
    "    \n",
    "    model = _build_model(config, model_dir, drop_rate=0.75)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.8, \n",
    "                                  patience=2, \n",
    "                                  #min_lr=0.000001,\n",
    "                                  verbose=1)\n",
    "    \n",
    "    filepath= os.path.join(model_dir, \"best_val_f1_model.h5\")\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True, mode='max')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=1) #TODO: 10\n",
    "    callback_list = [reduce_lr, early_stopping, checkpoint]\n",
    "    history = model.fit(x=[df_train_q1_emb_limit, df_train_q2_emb_limit],\n",
    "                  y=df_train_label_limit, \n",
    "                  batch_size=config['batch_size'], \n",
    "                  epochs=5, #TODO: config['nb_epochs']\n",
    "                  verbose=1, \n",
    "                  validation_data = ([df_val_q1_emb_limit,df_val_q2_emb_limit], df_val_label_limit),\n",
    "                  callbacks=callback_list)#\n",
    "    return (history,model)\n",
    "\n",
    "start = time.time()\n",
    "config = du.get_config(train_dataset_path, test_size=test_size, val_size=val_size, embedding_dimension=emb_dim)\n",
    "history, model = train_model() \n",
    "end = time.time()\n",
    "print(\"Total time\", (end - start))\n",
    "du.plot_model_accuracy(history,modelDir=model_dir, hasF1=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test_q1_emb.shape (4043, 32, 300)\n",
      "df_test_q2_emb.shape (4043, 32, 300)\n",
      "df_test_label.shape (4043,)\n",
      "4043/4043 [==============================] - 17s 4ms/step\n",
      "\n",
      " Test results \n",
      "\n",
      "{\n",
      "    \"acc\": 0.8595102647286725,\n",
      "    \"f1\": 0.8072119986040888,\n",
      "    \"loss\": 0.4305970115942322,\n",
      "    \"precision\": 0.7904351213442462,\n",
      "    \"recall\": 0.8398028106463006\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "df_test_q1_emb, df_test_q2_emb, df_test_label  = du.load_dataset(testDataset,w2v, config)\n",
    "\n",
    "\n",
    "print(\"df_test_q1_emb.shape\",df_test_q1_emb.shape)\n",
    "print(\"df_test_q2_emb.shape\", df_test_q2_emb.shape)\n",
    "print(\"df_test_label.shape\", df_test_label.shape)\n",
    "\n",
    "result = model.evaluate(x=[df_test_q1_emb, df_test_q2_emb], y=df_test_label)\n",
    "metrc_names = model.metrics_names\n",
    "result = {k:v for k,v in zip(metrc_names, result)}\n",
    "\n",
    "\n",
    "result = json.dumps(result, sort_keys=True, indent=4)\n",
    "print(\"\\n Test results \\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
