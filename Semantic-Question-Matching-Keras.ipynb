{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic-Question-Matching-Keras\n",
    "\n",
    "You can download data from: http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv \n",
    "\n",
    "Dataset info: https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs \n",
    "\n",
    "Blog post about Quora model: https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import sqmutils.data_utils as du\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "pathToDataset = \"/home/elkhand/datasets/Quora/data/quora_duplicate_questions.tsv\"\n",
    "embedding_path = \"/home/elkhand/datasets/glove-vectors/glove.twitter.27B.100d.txt\"\n",
    "train_dataset_path = pathToDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(pathToDataset, sep='\\t', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/dev/test set\n",
    "- train 98%\n",
    "- dev 1%\n",
    "- test 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n",
      " {'train_dataset_path': '/home/elkhand/datasets/Quora/data/quora_duplicate_questions.tsv', 'val_dataset_path': None, 'test_size': 0.01, 'seed': 7, 'is_debug_on': False} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " trainDataset Label distribution:  is_duplicate\n",
      "0    249949\n",
      "1    146292\n",
      "Name: is_duplicate, dtype: int64 \n",
      "\n",
      "\n",
      " valDataset Label distribution:  is_duplicate\n",
      "0    2525\n",
      "1    1478\n",
      "Name: is_duplicate, dtype: int64 \n",
      "\n",
      "\n",
      " testDataset Label distribution:  is_duplicate\n",
      "0    2550\n",
      "1    1493\n",
      "Name: is_duplicate, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First split dataset into train(99%) and test(1%)\n",
    "config = du.get_config(train_dataset_path,test_size=0.01)\n",
    "trainDataset, testDataset = du.create_train_test_split(config)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Second, split train dataset into train (98%) and val (1%) datasets\n",
    "trainDataset, valDataset = du.create_train_test_split_from_df(trainDataset, config)\n",
    "\n",
    "\n",
    "print(\"\\n\",\"trainDataset Label distribution: \", trainDataset.groupby('is_duplicate').is_duplicate.count(), \"\\n\")\n",
    "print(\"\\n\",\"valDataset Label distribution: \", valDataset.groupby('is_duplicate').is_duplicate.count() , \"\\n\")\n",
    "print(\"\\n\",\"testDataset Label distribution: \", testDataset.groupby('is_duplicate').is_duplicate.count() , \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings\n",
    "\n",
    "We will be using GloVe twitter 100D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vectors path /home/elkhand/datasets/glove-vectors/glove.twitter.27B.100d.txt\n",
      "embedding size : 1193514\n",
      "embedding dimension : (100,)\n",
      "Total time passed:  55.98992872238159\n"
     ]
    }
   ],
   "source": [
    "print(\"word vectors path\", embedding_path)\n",
    "start = time.time()\n",
    "w2v = du.load_embedding(embedding_path)\n",
    "end = time.time()\n",
    "print(\"Total time passed: \", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\n",
      " {'embedding_dimension': 100, 'max_seq_len': 40, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'hidden_layer_dim': 40, 'batch_size': 40, 'nb_epochs': 100} \n",
      "\n",
      "embedding size : 1193514\n",
      "embedding dimension : (100,)\n",
      "Sample words from word2vec:  ['<user>', '.', ':', 'rt', ',', '<repeat>', '<hashtag>', '<number>', '<url>', '!'] ['ｶﾞﾘｶﾞﾘ', 'ｷｲ', 'ｹﾞｼｯ', 'ﾃﾍﾍﾟﾛｯ', 'ﾃﾞﾓ', 'ﾊﾞｲﾊﾞｰｲ', 'ﾊﾟﾝﾁ', 'ﾔﾒﾀﾏｴ', 'ﾖｲｼｮｯ', 'ﾟﾟﾟｵﾔｽﾐｰ']\n"
     ]
    }
   ],
   "source": [
    "def _build_model(num_of_classes, config): \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(None, config['embedding_dimension'])))\n",
    "    model.add(Bidirectional(GRU(config['hidden_layer_dim'], return_sequences=True, \\\n",
    "                dropout=config['dropout'], recurrent_dropout=config['recurrent_dropout']), merge_mode='concat'))\n",
    "    model.add(Bidirectional(GRU(config['hidden_layer_dim'], dropout=config['dropout'],\\\n",
    "                                recurrent_dropout=config['recurrent_dropout']), merge_mode='concat'))\n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "       \n",
    "def train_model(X_train, y_train, X_val, y_val, num_of_classes, config):\n",
    "    print('X_train shape : %s' % (X_train.shape,))\n",
    "    print('y_train shape : %s' % (y_train.shape,))\n",
    "    print('X_val shape : %s' % (X_val.shape,))\n",
    "    print('y_val shape : %s' % (y_val.shape,))\n",
    "    print('number of classes : %d' % num_of_classes) \n",
    "    model = _build_model(num_of_classes, config)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.2, \n",
    "                                  patience=5, \n",
    "                                  min_lr=0.001)\n",
    "    # checkpoint\n",
    "    filepath=\"model/adidas-may-29.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25)#10\n",
    "    callback_list = [reduce_lr, checkpoint] # , early_stopping\n",
    "    history = model.fit(x=X_train,\n",
    "                  y=y_train, \n",
    "                  batch_size=config['batch_size'], \n",
    "                  epochs=config['nb_epochs'], \n",
    "                  verbose=1, \n",
    "                  validation_data = (X_val, y_val),\n",
    "                  shuffle=True,\n",
    "                  callbacks=callback_list)#\n",
    "    \n",
    "    val_acc_list = history.history['val_acc']\n",
    "    best_val_acc =  max(val_acc_list)\n",
    "    filename = \"intent\" \n",
    "    filename = \"model/\" + dt.generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "    os.rename(filepath, filename)\n",
    "    return history\n",
    "\n",
    "def train_helper(dfTrain, dfVal, config, shouldShuffleTrainDataset=True):\n",
    "    class_to_index = {}\n",
    "    index_to_class = {}\n",
    "    dfTrain = shuffle(dfTrain)\n",
    "    X_train, y_train_index, num_of_classes, class_to_index, index_to_class = dt.load_dataset_StratifiedKFold(dfTrain, w2v, config, class_to_index, index_to_class)\n",
    "    y_train = dt.convert_index_to_one_hot(y_train_index, num_of_classes) \n",
    "        \n",
    "    print(\"dfTrain.head(10) \\n\", dfTrain.head(10))\n",
    "#     print(\"dfTrain.tail(10) \\n\", dfTrain.tail(10))\n",
    "    print(\"\\n\",\"Train label distribution: \\n\",dfTrain.groupby('label').label.count())\n",
    "#         print(\"num_of_classes\", num_of_classes)\n",
    "#         print(\"class_to_index\", class_to_index)\n",
    "#         print(\"index_to_class\", index_to_class)\n",
    "        \n",
    "    print(\"dfVal.head(10)\",dfVal.head(10))\n",
    "#     print(\"dfVal.tail(10)\",dfVal.tail(10))\n",
    "        \n",
    "    X_val, y_val_index, _, _, _ = dt.load_dataset_StratifiedKFold(dfVal, w2v, config, class_to_index, index_to_class)\n",
    "    y_val = dt.convert_index_to_one_hot(y_val_index, num_of_classes) \n",
    "    print(\"\\n\",\"Val label distribution: \",dfVal.groupby('label').label.count())\n",
    "        \n",
    "    # Train model\n",
    "    history = train_model(X_train, y_train, X_val, y_val, num_of_classes, config)\n",
    "    val_acc_list = history.history['val_acc']\n",
    "    best_val_acc =  max(val_acc_list)\n",
    "    return [history, best_val_acc]\n",
    "\n",
    "\n",
    "def predict(w2v):\n",
    "    model = keras.models.load_model('model/intent_model_ricodataset.h5')\n",
    "    sentence = \"www.google.com\"\n",
    "    X_train = []\n",
    "    X_train.append(get_sequence_embedding(sentence.split(\" \"),w2v, max_seq_len))\n",
    "    X_train = np.array(X_train)\n",
    "    print(X_train.shape)\n",
    "    result = model.predict(X_train)\n",
    "    print(result)\n",
    "    print(index_to_class)\n",
    "    for i in index_to_class:\n",
    "        print(\"%s : %.3f%%\" % (index_to_class[i], result[0][i] * 100))\n",
    "\n",
    "def text_pre_processing(document):\n",
    "    document = text.text_to_word_sequence(document, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "    print(document)\n",
    "    return document\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    conf = {}\n",
    "    conf[\"embedding_dimension\"] = 100# 300 #100\n",
    "    conf[\"max_seq_len\"] = 40\n",
    "    conf[\"dropout\"] = 0.3\n",
    "    conf[\"recurrent_dropout\"] = 0.3\n",
    "    conf[\"hidden_layer_dim\"] = 40\n",
    "    conf[\"batch_size\"] = 40\n",
    "    conf[\"nb_epochs\"] = 100 #300\n",
    "    print(\"config\\n\",conf,\"\\n\")\n",
    "    return conf\n",
    "    \n",
    "\n",
    "def main():\n",
    "    config = get_config()\n",
    "    if w2v is not None:\n",
    "        print('embedding size : %d' % len(w2v))\n",
    "        print('embedding dimension : %s' % (w2v['apple'].shape,))\n",
    "        print(\"Sample words from word2vec: \", list(w2v.keys())[:10], list(w2v.keys())[-10:])\n",
    "#         history, best_val_acc = train_helper(dfTrain, dfVal, config)\n",
    "#         pt.plot_model_accuracy(history,\"model/\", isF1Enabled)\n",
    "#         print(\"best_val_acc\",best_val_acc)\n",
    "    # predict()\n",
    "    # text_pre_processing(\"hello&nbsp;hi\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
